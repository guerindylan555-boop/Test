
import { GoogleGenAI, Modality } from "@google/genai";
import { Gender, GarmentType, Pose } from '../types';

if (!process.env.API_KEY) {
    console.warn("API_KEY environment variable not set. Using a placeholder. Please set your Gemini API key.");
}
const ai = new GoogleGenAI({ apiKey: process.env.API_KEY || "YOUR_API_KEY_HERE" });

const fileToGenerativePart = async (file: File) => {
    const base64EncodedDataPromise = new Promise<string>((resolve) => {
        const reader = new FileReader();
        reader.onloadend = () => resolve((reader.result as string).split(',')[1]);
        reader.readAsDataURL(file);
    });
    return {
        inlineData: { data: await base64EncodedDataPromise, mimeType: file.type },
    };
};

const buildPrompt = (
    garmentType: GarmentType,
    gender: Gender,
    environment: string,
    pose: Pose | string,
    extraInstructions: string,
    hasModelRef: boolean,
    hasEnvRef: boolean,
) => {
    const poseDescriptions: Record<string, string> = {
        [Pose.Face]: "Front-facing mirror view; squared shoulders; phone centered.",
        [Pose.ThreeQuarter]: "Three-quarter view toward the mirror; torso angled; weight slightly on one leg.",
        [Pose.FromTheSide]: "Profile view toward the mirror; ensure torso and garment remain visible.",
        [Pose.Random]: "A dynamic and natural pose selected at random."
    };
    const poseDescription = poseDescriptions[pose] || pose;

    return `A photorealistic mirror selfie with an amateur smartphone look. The person is holding a black iPhone 16 Pro, which occludes their face but shows the garment clearly.

**GARMENT FIDELITY (Strict):**
The person MUST be wearing the uploaded garment from the first image. Preserve the exact silhouette, color, fabric texture, print scale, print alignment, closures, and any logos from the garment image.
This is a ${garmentType} garment.

**PERSON:**
${hasModelRef ? 'The person in the photo must strongly resemble the person from the provided reference image for the model, including hair, build, and general features.' : `The person is a ${gender}.`}

**ENVIRONMENT:**
${hasEnvRef ? 'The mirror selfie is taken in a scene that matches the lighting, angle, color palette, shadows, and depth of field of the provided environment reference image.' : `The scene is: ${environment}.`}

**POSE:**
Pose description: ${poseDescription}

${extraInstructions ? `**EXTRA INSTRUCTIONS:**\n${extraInstructions}\n` : ''}
**QUALITY & SAFETY:**
Ensure realistic hand anatomy. The face should be realistic, even if partially occluded. No explicit content, text, or watermarks. The pose must not obscure important details of the garment.`;
};

export const generateOnModelImage = async (
    garmentImage: File,
    options: {
        garmentType: GarmentType,
        gender: Gender,
        environment: string,
        pose: Pose | string,
        extraInstructions: string
    },
    modelRefImage?: File,
    envRefImage?: File
): Promise<string> => {
    const { garmentType, gender, environment, pose, extraInstructions } = options;
    const prompt = buildPrompt(garmentType, gender, environment, pose, extraInstructions, !!modelRefImage, !!envRefImage);

    const garmentPart = await fileToGenerativePart(garmentImage);
    const modelRefPart = modelRefImage ? await fileToGenerativePart(modelRefImage) : null;
    const envRefPart = envRefImage ? await fileToGenerativePart(envRefImage) : null;

    const parts = [{ text: prompt }, garmentPart];
    if (modelRefPart) parts.push(modelRefPart);
    if (envRefPart) parts.push(envRefPart);

    try {
        const response = await ai.models.generateContent({
            model: 'gemini-2.5-flash-image-preview',
            contents: { parts },
            config: {
                responseModalities: [Modality.IMAGE, Modality.TEXT],
            },
        });

        const imagePart = response.candidates?.[0]?.content?.parts.find(p => p.inlineData);
        if (imagePart?.inlineData) {
            return `data:${imagePart.inlineData.mimeType};base64,${imagePart.inlineData.data}`;
        }
        throw new Error('No image was generated by the model.');
    } catch (error) {
        console.error("Error generating on-model image:", error);
        throw new Error("Failed to generate image. The model might be unavailable or the request was blocked.");
    }
};

export const generateTextualDescription = async (prompt: string): Promise<string> => {
     try {
        const response = await ai.models.generateContent({
            model: 'gemini-2.5-flash',
            contents: prompt,
        });
        return response.text;
    } catch (error) {
        console.error("Error generating text:", error);
        throw new Error("Failed to generate text description.");
    }
};
